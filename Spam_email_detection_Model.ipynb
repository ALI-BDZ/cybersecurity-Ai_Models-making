{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# i try to make names for the collumns based on the collumn names in the spambase names \n",
    "word_features = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\", \n",
    "    \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\", \n",
    "    \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \n",
    "    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\", \n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \n",
    "    \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \n",
    "    \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \n",
    "    \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\", \n",
    "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \n",
    "    \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\"\n",
    "]\n",
    "\n",
    "\n",
    "char_features = [\n",
    "    \"char_freq_semicolon\", \"char_freq_parenthesis\", \"char_freq_bracket\", \n",
    "    \"char_freq_exclamation\", \"char_freq_dollar\", \"char_freq_hash\"\n",
    "]\n",
    "\n",
    "\n",
    "capital_features = [\n",
    "    \"capital_run_length_average\", \"capital_run_length_longest\", \"capital_run_length_total\"\n",
    "]\n",
    "\n",
    "target = [\"is_spam\"]\n",
    "\n",
    "columnnames=word_features+char_features+capital_features+target\n",
    "\n",
    "\n",
    "data=pd.read_csv('spambase.data', header=None,names=columnnames)\n",
    "\n",
    "X=data.drop('is_spam',axis=1)\n",
    "y=data['is_spam']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here i orgnized my dataset  using pandas library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i split the data for train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "test_log_loss = log_loss(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# we used this pour answer the third question its for validation of the model we calculatr accuracy \n",
    "\n",
    "\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Number of yprobpred: {y_pred_proba}\")\n",
    "\n",
    "print(f\"Test Log Loss: {test_log_loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV score: {cv_scores.mean():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nModel Coefficients:\")\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_[0]\n",
    "})\n",
    "print(coefficients.sort_values(by='Coefficient', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use regression logistic in this case and when u use it we have to use log loss function (sigmoid (Z)+log loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV score: {cv_scores.mean():.4f}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and  a technique for validation we used this sklearn.model_selection we take our dataset we split it to a number for example cv=5 and we train our dataset 5 times to see the model robusteness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
